import torch
import os
from matplotlib import pyplot as plt

from drl.rl_envs import cc_reward_distributions

from ddpm.model.main.base import OneShotDDPMReverseProcess
from ddpm.model.transition import VectoralResidualModel
from ddpm.model.embedding_reprs import SmoothEmbeddingBlockWithExtraEmbeddings


def make_reward_distributions(all_reward_distribution_configs: list):

    assert len(all_reward_distribution_configs) in [1, 2], "Cannot visualise dim(r) > 2 right now!"

    all_reward_distributions = []
    all_target_rewards_distribution_probs_x_axes = []
    all_target_rewards_distribution_probs = []
    all_numpy_friendly_reward_distributions = []

    for reward_distribution_config in all_reward_distribution_configs:
        all_target_rewards_distribution_probs_x_axes.append(torch.linspace(reward_distribution_config.reward_distribution_min, reward_distribution_config.reward_distribution_max, 50))
        all_reward_distributions.append(getattr(cc_reward_distributions, reward_distribution_config.reward_distribution_name)(**reward_distribution_config.reward_distribution_kwargs.dict))
        all_target_rewards_distribution_probs.append(all_reward_distributions[-1].log_prob(all_target_rewards_distribution_probs_x_axes[-1]).exp())
        all_numpy_friendly_reward_distributions.append(lambda x: all_reward_distributions[-1].cdf(torch.tensor(x)).cpu().numpy())

    return (
        all_reward_distributions,
        all_target_rewards_distribution_probs_x_axes,
        all_target_rewards_distribution_probs,
        all_numpy_friendly_reward_distributions,
    )



def make_model(task_time_embedding_size: int, diffusion_time_embedding_size: int, sigma2x_schedule, time_between_cs_and_us: int, nr: int, device: str):
    input_model = SmoothEmbeddingBlockWithExtraEmbeddings(
        total_time = time_between_cs_and_us + 1,    # Need one state for before (0), and one state for after (<time_between_cs_and_us>)
        time_embedding_dim = task_time_embedding_size,
        num_extra_embeddings = 1,       # Don't want the s=-1 state to have a smooth continuation of the others
        device = device
    )
    input_model.network_input_size = task_time_embedding_size


    residual_model = VectoralResidualModel(
        state_space_size = nr,                                  # Value function
        recurrence_hidden_layers = [16, 16, 16],                # Smallish network
        input_size = task_time_embedding_size,                  # Task time embeddings will be generated by the time embedding input model
        time_embedding_size = diffusion_time_embedding_size,    # Diffusion time embeddings
        nonlin_first = False                                    # Value function might be negative
    )


    ddpm_model = OneShotDDPMReverseProcess(
        sample_shape = [nr],                                    # Value function
        sigma2xt_schedule = sigma2x_schedule,                   # 20 diffusion steps
        residual_model = residual_model,                        # Fully connected
        input_model = input_model,                              # Fed with index values (integers)
        time_embedding_size = diffusion_time_embedding_size,    # Diffusion time embeddings
        device = device,
    )
    ddpm_model.to(device)

    return ddpm_model



def make_classical_conditioning_stimuli(wait_time: int, time_between_cs_and_us: int, time_after_us: int, batch_size: int, num_diffusion_timesteps: int, device: str):

    stimulus_array = torch.concat(
        [
            torch.ones(wait_time) * -1,
            torch.arange(time_between_cs_and_us),
            torch.ones(time_after_us) * -1
        ], dim = 0
    ).to(device).to(int)
    num_task_timesteps = stimulus_array.shape[0]

    s_t = stimulus_array[:-1].unsqueeze(0).repeat(batch_size, 1)
    s_t_plus_1 = stimulus_array[1:].unsqueeze(0).repeat(batch_size, 1)
    s_all = stimulus_array.unsqueeze(0).repeat(batch_size, 1)

    # XXX: This needs to have num_diffusion_timesteps in shape because the TimeEmbeddingBlock should not be used as an input model!
        # It's a hack that should be removed...
    input_t = s_t# .unsqueeze(-1).repeat(1, 1, num_diffusion_timesteps)
    input_t_plus_1 = s_t_plus_1# .unsqueeze(-1).repeat(1, 1, num_diffusion_timesteps)
    input_all = s_all# .unsqueeze(-1).repeat(1, 1, num_diffusion_timesteps)

    return (
        stimulus_array,
        num_task_timesteps,
        s_t,
        s_t_plus_1,
        s_all,
        input_t,
        input_t_plus_1,
        input_all,
    )


def plot_model_schedules(ddpm_model, path):

    sch_fig, sch_axes = plt.subplots(2, 1, figsize=(5, 8))
    sch_axes[0].plot(
        ddpm_model.sigma2xt_schedule.cpu().numpy(), label="sigma2xt_schedule", alpha=0.4
    )
    sch_axes[0].plot(ddpm_model.a_t_schedule.cpu().numpy(), label="a_t_schedule", alpha=0.4)
    sch_axes[0].plot(
        ddpm_model.root_b_t_schedule.cpu().numpy(), label="root_b_t_schedule", alpha=0.4
    )
    sch_axes[0].plot(
        ddpm_model.noise_scaler_schedule.cpu().numpy(),
        label="noise_scaler_schedule",
        alpha=0.4,
    )
    sch_axes[0].plot(
        ddpm_model.base_samples_scaler_schedule.cpu().numpy(),
        label="base_samples_scaler_schedule",
        alpha=0.4,
    )
    sch_axes[0].plot(
        ddpm_model.residual_scaler_schedule.cpu().numpy(),
        label="residual_scaler_schedule",
        alpha=0.4,
    )
    sch_axes[0].legend()

    sch_axes[1].set_title("Time embeddings")
    sch_axes[1].imshow(ddpm_model.time_embeddings.time_embs.detach().cpu().numpy().T)

    sch_fig.savefig(path)


