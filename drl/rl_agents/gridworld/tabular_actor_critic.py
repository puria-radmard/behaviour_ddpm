from drl.rl_envs.gridworld.cell_types import TransitionInformationBatch

import torch
from torch import nn
from torch import Tensor as _T

from typing import Union, Optional, Tuple
from dataclasses import dataclass

from abc import ABC, abstractmethod


@dataclass
class TD1ErrorUpdateInformation:
    previous_q: _T  # Q(s, a)
    next_q: _T      # Q(s', a')
    target_q: _T    # r + gamma * Q(s', a') or r if terminal



class GridWorldCriticModelBase(nn.Module, ABC):
    
    def __init__(self, num_states: int, discount_factor: float, actions_per_state: int) -> None:
        super().__init__()
        self.discount_factor = discount_factor

    @abstractmethod
    def get_q_values(self, states: _T, actions: Optional[_T] = None) -> _T:
        """
        states (and actions) of size [B]
        
        if actions == None: return [B, A]
        else: return [B]
        """
        raise NotImplementedError

    def get_targets(self, transition_batch: TransitionInformationBatch, next_actions: _T):
        """
        Default to a TD1 error

        Currently Q(s, a) derived from same parameters as Q(s', a')

        transition_batch contains information about s, a, r, s'
        next_actions gives a', which may be smaller than a, given terminal transitions
        """
        start_state_values = self.get_q_values(transition_batch.old_cell_ids, transition_batch.actions)

        targets = transition_batch.transition_rewards
        non_terminal_mask = ~transition_batch.terminal

        if non_terminal_mask.any():
            with torch.no_grad():
                end_state_values = self.get_q_values(transition_batch.new_cell_ids[non_terminal_mask], next_actions)
                targets[non_terminal_mask] += (self.discount_factor * end_state_values)

        return TD1ErrorUpdateInformation(
            previous_q=start_state_values,
            next_q=end_state_values,
            target_q=targets
        )
    
    def get_loss(self, td1_info: TD1ErrorUpdateInformation) -> _T:
        """
        Used to perform, which is just Q <- Q + learning_rate * td1_info.td1_error for the basic case!
        """
        return 0.5 * (td1_info.target_q.detach() - td1_info.previous_q).square().mean()




class TabularGridWorldCriticModel(GridWorldCriticModelBase):

    def __init__(self, num_states: int, discount_factor: float, actions_per_state: int = 4) -> None:
        super().__init__(num_states, discount_factor, actions_per_state)
        self.register_parameter('values', torch.nn.Parameter(6 * torch.ones(num_states, actions_per_state).float()))

    def get_q_values(self, states: _T, actions: Optional[_T] = None) -> _T:
        if actions is None:
            return self.values[states]
        else:
            return self.values[states, actions]



class DistributionalGridWorldCriticModelBase(GridWorldCriticModelBase, ABC):
    
    @abstractmethod
    def get_q_values(self, states: _T, actions: Optional[_T] = None, num_samples: int = 1024) -> _T:
        """
        states (and actions) of size [B]
        
        if actions == None: return [B, A]
        else: return [B]
        """
        raise NotImplementedError





class EmbeddingInputDDPMCriticModel(DistributionalGridWorldCriticModelBase):
    """
    DDPM takes in an embedding for state, and another for action
    Outputs a set of samples for the dstate value, 
    """
        



class GridWorldActorModel(nn.Module, ABC):

    requires_q_values: bool

    def __init__(self, num_states: int, actions_per_state: int = 4) -> None:
        super().__init__()
        self.num_states = num_states
        self.actions_per_state = actions_per_state

    @staticmethod
    @torch.no_grad()
    def sample_actions(pmfs: _T) -> _T:
        cdfs = pmfs.cumsum(-1)
        u = torch.rand(len(cdfs), 1)
        choices = (u > cdfs).sum(-1)
        return choices

    @abstractmethod
    def choose_actions(self, current_states: _T, *args) -> Union[_T, _T]:
        """
        Returns probabilties over actions, then the choices made
        
        When doing eps-greedy, just pass the state action value samples here
        """
        raise NotImplementedError


    def get_loss(
        self, 
        action_probs: _T,
        action_choices: _T,
        current_state_action_values: _T
    ) -> _T:
        """
        Ascent is done on Q(s, a) * log\pi(a|s), so we return the negative

        current_state_action_values gives all action values for (s, a) which we are leaving
            shaped [B]

            Should be detached!
        
        action_probs is the same shape, generated by self.choose_actions
        """
        relevant_probs = action_probs[torch.arange(len(action_probs)), action_choices]
        relevant_log_probs = relevant_probs.log()
        return - (current_state_action_values.detach() * relevant_log_probs).mean()




class TabularActorModel(GridWorldActorModel):

    requires_q_values = False

    def __init__(self, num_states: int, actions_per_state: int = 4) -> None:
        super().__init__(num_states, actions_per_state)
        self.register_parameter('logits', torch.nn.Parameter(2.0 * torch.randn(num_states, actions_per_state)))
    
    def choose_actions(self, current_states: _T) -> Tuple[_T, _T]:
        if len(current_states) == 0:
            return torch.tensor([]).unsqueeze(1).reshape(-1, self.actions_per_state), torch.tensor([])
        pmfs = self.logits[current_states].softmax(-1)
        choices = self.sample_actions(pmfs)
        return pmfs, choices



class EpsGreedyActorModel(GridWorldActorModel):

    requires_q_values = True

    def __init__(self, num_states: int, actions_per_state: int = 4) -> None:
        super().__init__(num_states, actions_per_state)
        self.epsilon = 0.1
    
    def choose_actions(self, current_states: _T, state_value_vectors: _T) -> Tuple[_T, _T]:
        """
        current_states of shape [B], indices
        state_value_vectors of shape [B, A]
        """
        choices = state_value_vectors.argmax(-1)
        pmfs = torch.ones_like(state_value_vectors) * (self.epsilon / self.actions_per_state)
        pmfs[torch.arange(state_value_vectors.shape[0]), choices] += (1-self.epsilon)
        choices = self.sample_actions(pmfs)
        return pmfs, choices

    def get_loss(
        self, 
        action_probs: _T,
        action_choices: _T,
        current_state_action_values: _T
    ) -> _T:
        return torch.tensor(0.0)

